#!/usr/bin/env bds
#vim: syntax=java

include "graphviz.bds"
include "filetable.bds"
include "log_parser.bds"
include "ENCODE_accession.bds"


help == report settings
url_base 	:= "" 		help URL base for output directory.


_padding_left	:= 20
_report_file  	:= ""
_report_header 	:= ""

init_report()


void init_report() {
	url_base = get_conf_val( url_base, 	["url_base"] )

	prefix := (title=="") ? "" : (title+"_")
	_report_file = "$out_dir/$prefix"+"report.html"
	_report_header = "$script_dir/html/rpt_header.html"

	if ( !is_cmd_line_arg_empty() ) {
		mkdir(rpt_aux_dir)
		sys cp --remove-destination $script_dir/html/jquery.treetable.* $rpt_aux_dir    # move report js/css files to rpt_aux_dir
	}

	print("\n\n== report settings\n")
	print( "URL root for output directory\t: $url_base\n" )
}

void report( string body ) { // HTML report

	wait
	
	html := _report_header.read()
	html += body
	html += "<br></body></html>"

	_report_file.write( html )
}

// filepath, node label, group, filetable hierachy
void add_file_to_report( string file, string label, string group, string hrchy ) {
	add_file_to_graph( file, label, group )
	add_file_to_table( [file], [hrchy] )
}

// qc_type must exist in qc_types = ["flagstat","dup","flagstat_filt","pbc","xcor","idr","ataqc"]
// if horz, groups show on the first row. otherwise on the first column
// if horz
//        | item1  | item2  | ...
// -------+--------+--------+-------
// group1 | val    | val    + ...
// group2 | val    | val    + ...
// ...
// else
//       | group1 | group2 | ...
// ------+--------+--------+-------
// item1 | val    | val    + ...
// item2 | val    | val    + ...
// ...

string html_table_multiple_logs( string table_name, bool horz, string qc_type, string[] groups, string[] logs ) {
	if ( logs.size() == 0 ) return ""
	html := "<div id='table_"+table_name.replace(" ","-")+\
		"'><b>$table_name</b><div style='padding-left: "+_padding_left+"px'>\n<table border='1'>\n"
	header := get_log_header( qc_type )
	if ( !header.size() ) header = get_header_multi_col_txt( logs[0] ) // for ataqc

	// split keys and descriptions
	string[] keys
	string[] descs
	for ( int j=0; j<header.size(); j++ ) {
		key_and_desc := split_by_first_chr( header[j], ":" )
		keys.add(key_and_desc[0])
		descs.add( key_and_desc.size()>1 ? key_and_desc[1] : key_and_desc[0])
	}

	// write table header
	if ( horz ) {
		html += "<tr>"
		if (logs.size()>1) html += "<th>&nbsp</th>"
		html += "<th nowrap align=left>"+descs.join("</th><th nowrap align=left>")+"</th></tr>\n"
		for ( int i=0; i<logs.size(); i++ ) {
			log := logs[i]
			map := parse_log( qc_type, log )
			html += "<tr>"
			if (logs.size()>1) html += "<td>"+html_link( log, groups[i] )+"</td>"
			for ( int j=0; j<header.size() ;j++ ) {				
				if ( map.hasKey(keys[j]) ) html += "<td>"+map{keys[j]}+"</td>"
				else html += "<td>&nbsp</td>"
			}
			html += "</tr>\n"
		}
	}
	else {
		if (logs.size()>1) html += "<tr><th nowrap align=left>&nbsp</th><th nowrap align=left>"+groups.join("</th><th nowrap align=left>")+"</th></tr>\n"
		for ( int j=0; j<header.size() ;j++ ) {
			html += "<tr><th nowrap align=left>"+descs[j]+"</th>"
			for ( int i=0; i<logs.size(); i++ ) {
				log := logs[i]
				map := parse_log( qc_type, log )
				if ( map.hasKey(keys[j]) ) html += "<td>"+map{keys[j]}+"</td>"
				else html += "<td>&nbsp</td>"
			}
			html += "</tr>\n"
		}
	}

	// add to ENCODE QC summary JSON
	for ( int i=0; i<logs.size(); i++ ) {
		log := logs[i]
		map := parse_log( qc_type, log )
		string[] vals
		for ( int j=0; j<header.size() ;j++ ) {
			if ( map.hasKey(keys[j]) ) vals.add(map{keys[j]})
			else vals.add("")
		}
		// add to ENCODE QC summary table
		_summary_qc.add( map_to_json_str( \
				{ "info"=>groups[i],"qc_type"=>qc_type,\
				"header"=>array_to_str(keys,"\\t"),\
				"contents"=>array_to_str(vals,"\\t") } ) ) 
	}

	html += "</table>"
	html += "</div></div><br>"
	return html
}

string html_help_pbc() {
	return "<div id='help-pbc'><p>" + "NRF (non redundant fraction) <br>" + \
			"PBC1 (PCR Bottleneck coefficient 1) <br>" + \
			"PBC2 (PCR Bottleneck coefficient 2) <br>" + \
			"PBC1 is the primary measure. Provisionally <br>" + \
			"<ul>" + \
			"<li>0-0.5 is severe bottlenecking</li>" + \
			"<li>0.5-0.8 is moderate bottlenecking </li>" + \
			"<li>0.8-0.9 is mild bottlenecking </li>" + \
			"<li>0.9-1.0 is no bottlenecking </li>" + \
			"</ul>" + \
			"</p></div>"
}

string html_help_xcor() {
	return "<div id='help-xcor'><p>" + "Normalized strand cross-correlation coefficient (NSC) = col9 in outFile <br>" + \
			"Relative strand cross-correlation coefficient (RSC) = col10 in outFile <br>" + \
			"Estimated fragment length = col3 in outFile, take the top value <br>" + \
			"Important columns highlighted, but all/whole file can be stored for display <br>" + \
			 "</p><br>"
}

// WashU Epigenome browser datahub (json format)
string html_epg_browser_viz( string[] files, string[] types, string[] names, string species ) {
	if ( files.size() == 0 ) return ""

	warn := (url_base=="") ? "(add <i>-url_base [URL_ROOT_DIR_FOR_OUT_DIR]</i> to the command line.)" : ""
	html := "<div id='viz'><b>Visualization $warn </b><div style='padding-left: "+_padding_left+"px'>"
	json := "[ " // json array starts here

	for ( int i=0; i<files.size(); i++ ) {
		file := files[i]
		type := types[i]
		trackname := names[i].trim()

		// add url_base
		url 	:= _get_url( file )
		if ( type == "bigwig") {
			// deep pink
			json 	+= 	'{ "type": "bigwig", "name": "'+trackname+'", "url": "'+url+'", "mode": 1,' + \
					'  "qtc" : { "height": 30, "summeth":2, "smooth":3, "pr": 255, "pg": 20, "pb": 147,"thtype":1, "thmin":2, "thmax":40  } },'
		}
		else if ( type == "hammock") { // peak must be converted to hammock type (browser specific format)	
			if ( file.indexOf(".IDR") > -1 ) { // IDR narrowpeak has 11th (local IDR) and 12th (global IDR) columns
				json += ' { "type": "hammock", "name": "'+trackname+'", "url": "'+url+'", "mode": "barplot", "showscoreidx":4,' + \
					'   strokecolor:"#ff6600", scorescalelst:[{"type":1,"min":0,"max":40},{"type":0},{"type":0},{"type":0},{"type":0}],' + \
					'   scorenamelst:["signal value","P value (-log10)","Q value (-log10)","local IDR (-log10)","global IDR (-log10)"],' + \
					' "qtc": { "height" : 30, "summeth":2, "fontsize":"0pt","fontfamily":"sans-serif" } },'
			}
			else {
				json += ' { "type": "hammock", "name": "'+trackname+'", "url": "'+url+'", "mode": "barplot", "showscoreidx":1,' + \
					'   strokecolor:"#ff6600", scorescalelst:[{"type":1,"min":0,"max":40},{"type":0},{"type":0},{"type":0}],' + \
					'   scorenamelst:["signal value", "P value (-log10)","Q value (-log10)"],' + \
					' "qtc": { "height" : 30, "summeth":2, "fontsize":"0pt","fontfamily":"sans-serif" } },'
			}
		}
	}

	// add ref genome track
	json += '{ "type":"native_track", "list":[ { "name":"refGene", "mode":"full", } ] },'
	json += " ]" // json array ends here
	prefix := title=="" ? "" : (title+"_")
	json_file := "$out_dir/$prefix"+"tracks.json"
	json_file.write( json )
	json_url := _get_url( json_file )
	viz_url	 := "http://epigenomegateway.wustl.edu/browser/?genome=" + species + "&tknamewidth=275&datahub=" + json_url
	json_rel_path := get_rel_path( json_file )

	html += "<a href=$viz_url target='_blank'>Visualize</a>&nbsp&nbsp"
	html += "<a href=$json_rel_path target='_blank'>JSON (datahub)</a>"
	html += "</div></div><br>\n\n"

	return html
}

string html_link( string path, string name ) {
	return "<a href='" + get_rel_path( path ) + "' target='_blank'>$name</a><br>"
}

string html_img( string path, int width, string cap ) {
	return "<figure style='display: inline-block;'><img src='" + get_rel_path( path ) + \
		"' width='$width'><figcaption style='text-align: center;'><b>$cap</b></figcaption></figure>"
}

string pdf_to_png( string pdf ) { 
	png 	:= rm_ext( pdf, "pdf" ) + ".png"

	// needs ghostscript installed
	taskName := "pdf2png"
	system := "local" // do not use cluster engine for this task
	
	task ( png <- pdf ) {
		sys $shcmd_init
		sys gs -dFirstPage=1 -dLastPage=1 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -r110x110 \
			-dUseCropBox -dQUIET -dSAFER -dBATCH -dNOPAUSE -dNOPROMPT -sDEVICE=png16m \
			-sOutputFile=$png \
			-r144 $pdf
		//sys pdftoppm -png $pdf > $png # Requirements: poppler-utils (sudo apt-get install poppler-utils)
	}

	return png
}

string peak_to_hammock( string peak ) {
	prefix 	:= rm_ext( peak, ["gz"] )

	hammock := "$prefix.hammock"
	tmp 	:= "$prefix.tmp"
	// choose correct converter .py : narrowpeak (regionpeak), broadpeak, gappedpeak to hammock
	// they are under git_repo_root/utils/	
	conv := "$script_dir/utils/"
	if ( peak.toLower().indexOf( "regionpeak" ) > -1 || \
	     peak.toLower().indexOf( "narrowpeak" ) > -1 ) 	conv += "narrowpeak.py"
	else if ( peak.toLower().indexOf( "broadpeak" ) > -1 ) 	conv += "broadpeak.py"
	else if ( peak.toLower().indexOf( "gappedpeak" ) > -1 ) conv += "gappedpeak.py"
	else if ( peak.toLower().indexOf( "13-col.bed" ) > -1 ) conv += "narrowpeak_idr.py"
	else 							conv += "narrowpeak.py"

	in 	:= peak
	out 	:= hammock+".gz"

	taskName:= "peak2hammock"
	system := "local" // do not use cluster engine for this task

	task ( out<-in ) { // needs bgzip and tabix

		sys $shcmd_init
		sys zcat $peak | sed '/^\(chr\)/!d' | sort -k1,1V -k2,2n > $tmp
		//sys python $conv $tmp $hammock
		sys $conv $tmp $hammock
		sys rm -f $tmp
	}

	wait

	return out
}

string _get_url( string path ) {
	rel_path := get_rel_path( path )
	if ( rel_path == "" ) 	return ""
	else 			return url_base + "/" + get_rel_path( path )
}
